{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 과대적합(Overfitting)과 과소적합(Underfitting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 머신러닝의 근본적인 이슈는 **최적화와 일반화 사이의 줄다리기**\n",
    "    - **최적화**\n",
    "        - 가능한 **훈련 데이터에서 최고의 성능**을 얻으려고 모델을 조정하는 과정\n",
    "    - **일반화(Generalization)**\n",
    "        - 훈련된 모델이 **이전에 본 적 없는 데이터에서 얼마나 잘 수행되는지**를 의미\n",
    "        \n",
    "        \n",
    "- **모델을 만드는 목적은 좋은 일반화 성능을 얻기 위한 것**이지만 일반화 성능을 제어할 방법이 없으며 단지 훈련 데이터를 기반으로 모델을 조정할 수 밖에 없음\n",
    "- 따라서 **최적화를 하되 일반화가 잘 되도록 적당히 조절**하는 법을 알아야 함\n",
    "- 이는 최적화 과정에서 일어나는 **과대적합으로 인한 일반화 성능 저하를 막는 것**을 뜻함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **과소 적합(underfitting)**\n",
    "    - 훈련 데이터의 손실이 낮아질수록 테스트 데이터의 손실도 낮아짐\n",
    "    - 네트워크가 훈련 데이터의 특성을 모두 학습하지 못한 상태\n",
    "    - **일반화 성능**이 **좋아짐**\n",
    "    \n",
    "\n",
    "- **과대 적합(overfitting)**\n",
    "    - 훈련 데이터의 손실이 낮아질수록 테스트 데이터의 손실이 높아짐\n",
    "    - 네트워크가 **훈련 데이터에 특화된 패턴을 학습**하기 시작\n",
    "    - **일반화 성능**이 더이상 높아지지 않으며 오히려 **낮아짐**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Regularization**\n",
    "    - **과대 적합을 피하는 처리 과정**을 **regularization(규제)**이라 함\n",
    "    - **가장 좋은 방법은 더 많은 훈련데이터를 모으는 것**\n",
    "        - 더 많은 데이터에서 훈련된 모델은 일반화 성능이 더욱 뛰어남\n",
    "    - **데이터를 더 모을 수 없을 경우**의 차선책은 **모델이 수용할 수 있는 정보의 양을 조절하거나 저장할 수 있는 정보에 제약**을 가하는 것\n",
    "        - 네트워크가 **적은 수의 패턴만 기억할 수 있다면** 최적화 과정에서 **가장 중요한 패턴에 집중**하게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 네트워크 크기 축소\n",
    "\n",
    "- 과대적합을 막는 가장 단순한 방법은 **네트워크의 크기(학습 파라미터의 수)를 줄이는 것**\n",
    "    - 파라미터의 수는 **층의 수**, 각 **층의 유닛의 수**를 뜻함\n",
    "\n",
    "\n",
    "- **네트워크의 학습 파라미터 수**는 네트워크가 **학습할 수 있는 용량(capacity)**을 뜻하며 **용량을 제한**한다면 많은 양을 학습해 **overfitting되는 것을 제한**할 수 있음\n",
    "- 따라서 **너무 많은 용량과 충분하지 않은 용량 사이의 절충점**을 찾아야 함(데이터에 **알맞은 네트워크의 크기**를 찾는 것)\n",
    "\n",
    "\n",
    "- 일반적인 방법은 비교적 적은 수의 층과 파라미터에서 시작해 validation loss가 감소되기 시작할 때 까지 층이나 유닛의 수를 늘리는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화 리뷰 분류 모델로 비교\n",
    "\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# IMDB 데이터셋 로드\n",
    "# num_words=10000은 자주 사용하는 단어 1만개만 사용한다는 의미\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 10000) (25000, 10000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorization(sequences, words=10000):\n",
    "    result = np.zeros((len(sequences), words))\n",
    "    \n",
    "    for idx, sequence in enumerate(sequences):\n",
    "        result[idx, sequence] = 1.\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "train_data = vectorization(train_data)\n",
    "test_data = vectorization(test_data)\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 1s 49us/step - loss: 0.4848 - acc: 0.7978\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 1s 29us/step - loss: 0.2768 - acc: 0.9053\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 1s 29us/step - loss: 0.2053 - acc: 0.9296\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 1s 29us/step - loss: 0.1682 - acc: 0.9419\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 1s 29us/step - loss: 0.1442 - acc: 0.9502\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 1s 30us/step - loss: 0.1235 - acc: 0.9578\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 1s 29us/step - loss: 0.1091 - acc: 0.9636\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 1s 30us/step - loss: 0.0947 - acc: 0.9686\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 1s 29us/step - loss: 0.0815 - acc: 0.9737\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 1s 31us/step - loss: 0.0742 - acc: 0.9759\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 1s 31us/step - loss: 0.0594 - acc: 0.9824\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 1s 32us/step - loss: 0.0528 - acc: 0.9847\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 1s 30us/step - loss: 0.0457 - acc: 0.9873\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 1s 31us/step - loss: 0.0385 - acc: 0.9891\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 1s 30us/step - loss: 0.0324 - acc: 0.9914\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 1s 31us/step - loss: 0.0273 - acc: 0.9928\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 1s 30us/step - loss: 0.0217 - acc: 0.9948\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 1s 31us/step - loss: 0.0192 - acc: 0.9954\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 1s 31us/step - loss: 0.0155 - acc: 0.9966\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 1s 30us/step - loss: 0.0137 - acc: 0.9968\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model_origin = models.Sequential()\n",
    "model_origin.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model_origin.add(layers.Dense(16, activation='relu'))\n",
    "model_origin.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_origin.compile(optimizer='rmsprop',\n",
    "                     loss='binary_crossentropy',\n",
    "                     metrics=['acc'])\n",
    "\n",
    "hist_origin = model_origin.fit(train_data, train_labels, epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 1s 34us/step - loss: 0.5318 - acc: 0.8049\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 1s 31us/step - loss: 0.3389 - acc: 0.8994\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 1s 31us/step - loss: 0.2565 - acc: 0.9178\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 1s 30us/step - loss: 0.2116 - acc: 0.9296\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 1s 30us/step - loss: 0.1838 - acc: 0.9388\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 1s 31us/step - loss: 0.1628 - acc: 0.9449\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 1s 31us/step - loss: 0.1462 - acc: 0.9512\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 1s 30us/step - loss: 0.1327 - acc: 0.9562\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 1s 31us/step - loss: 0.1211 - acc: 0.9608\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 1s 31us/step - loss: 0.1120 - acc: 0.9638\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 1s 31us/step - loss: 0.1027 - acc: 0.9674\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 1s 31us/step - loss: 0.0948 - acc: 0.9703\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 1s 31us/step - loss: 0.0879 - acc: 0.9732\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 1s 30us/step - loss: 0.0810 - acc: 0.9753\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 1s 30us/step - loss: 0.0754 - acc: 0.9775\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 1s 29us/step - loss: 0.0694 - acc: 0.9803\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 1s 31us/step - loss: 0.0644 - acc: 0.9816\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 1s 31us/step - loss: 0.0587 - acc: 0.9839\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 1s 31us/step - loss: 0.0544 - acc: 0.9856\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 1s 30us/step - loss: 0.0493 - acc: 0.9876\n"
     ]
    }
   ],
   "source": [
    "model_small = models.Sequential()\n",
    "model_small.add(layers.Dense(6, activation='relu', input_shape=(10000,)))\n",
    "model_small.add(layers.Dense(6, activation='relu'))\n",
    "model_small.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_small.compile(optimizer='rmsprop',\n",
    "                     loss='binary_crossentropy',\n",
    "                     metrics=['acc'])\n",
    "\n",
    "hist_small = model_small.fit(train_data, train_labels, epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
