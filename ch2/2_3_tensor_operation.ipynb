{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 텐서 연산\n",
    "\n",
    "- 첫 예제에서의 Dense layer를 간단히 분석해보면 다음과 같다.\n",
    "\n",
    "\n",
    "- keras.layers.Dense(512, activation='relu')\n",
    "    - 입력으로 2D 텐서를 받아서 출력으로 2D 텐서를 반환하는 함수\n",
    "    - 더 자세히 풀어서 보면 다음과 같다.\n",
    "    \n",
    "    \n",
    "- output = relu(dot(W, input) + b)\n",
    "    - input(입력 2D 텐서), W(가중치 2D 텐서), bias(벡터)\n",
    "    - 총 3가지 연산을 함\n",
    "        1. input과 W의 dot product\n",
    "        2. 1의 결과(2D 텐서)와 b의 덧셈\n",
    "        3. ReLU 연산\n",
    "    \n",
    "    \n",
    "- ReLU 연산\n",
    "    - max(x, 0)\n",
    "    - 입력이 0보다 크면 그대로 반환\n",
    "    - 0보다 작으면 0을 반환\n",
    "    \n",
    "    \n",
    "***Sequential 클래스의 add() method에 Dense 클래스가 추가될 때, Dense 객체의 build() 메서드가 호출되며 가중치(kernel) W와 편향(bias) b가 생성되고, Dense 객체의 kernel과 bias 인스턴스 변수에 저장됨***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 원소별 연산 (element-wise operation)\n",
    "\n",
    "- relu()와 덧셈은 원소별 연산\n",
    "- 원소별 연산이란 텐서내의 원소끼리 이루어지는 연산을 뜻함\n",
    "- 넘파이에 원소별 연산이 이미 구현되어 있고 훨씬 빠름\n",
    "- 주로 * 연산자를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "[[-1 -4 -3]\n",
      " [-7 -3  6]\n",
      " [ 3 -5 -4]]\n",
      "b\n",
      "[[-9 -2 -2]\n",
      " [-8 -5 -3]\n",
      " [-8  3 -5]]\n",
      "\n",
      "relu(a)\n",
      "[[0 0 0]\n",
      " [0 0 6]\n",
      " [3 0 0]]\n",
      "\n",
      "add(a, b)\n",
      "[[-10  -6  -5]\n",
      " [-15  -8   3]\n",
      " [ -5  -2  -9]]\n",
      "\n",
      "numpy의 relu\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 6.]\n",
      " [3. 0. 0.]]\n",
      "\n",
      "numpy의 add\n",
      "[[-10  -6  -5]\n",
      " [-15  -8   3]\n",
      " [ -5  -2  -9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 직접 구현한 relu()\n",
    "def naive_relu(x):\n",
    "    assert len(x.shape) == 2\n",
    "    \n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = max(x[i, j], 0)\n",
    "            \n",
    "    return x\n",
    "\n",
    "\n",
    "# 직접 구현한 add()\n",
    "def naive_add(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert x.shape == y.shape\n",
    "    \n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[i, j]\n",
    "            \n",
    "    return x\n",
    "\n",
    "\n",
    "a = np.random.randint(-10, 10, (3, 3))\n",
    "b = np.random.randint(-10, 10, (3, 3))\n",
    "\n",
    "print(\"a\")\n",
    "print(a)\n",
    "print(\"b\")\n",
    "print(b)\n",
    "print(\"\")\n",
    "\n",
    "print(\"relu(a)\")\n",
    "print(naive_relu(a))\n",
    "print(\"\")\n",
    "\n",
    "print(\"add(a, b)\")\n",
    "print(naive_add(a, b))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# numpy의 구현된 maximum(relu()와 동일한 기능)\n",
    "print(\"numpy의 relu\")\n",
    "print(np.maximum(a, 0.))\n",
    "print(\"\")\n",
    "\n",
    "# numpy의 구현된 add()\n",
    "print(\"numpy의 add\")\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2 브로드캐스팅 (broadcasting)\n",
    "- 작은 텐서가 큰 텐서의 크기에 맞춰지는 것을 브로드캐스팅이라 함\n",
    "- 브로드캐스팅의 2단계\n",
    "    1. 큰 텐서의 ndim에 맞도록 작은 텐서에 축(브로드캐스팅 축이라 함)이 추가됨\n",
    "    2. 작은 텐서가 새 축을 따라서 큰 텐서의 크기에 맞도록 반복됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a : (10, 5)\n",
      "b : (5,)\n",
      "\n",
      "a+b (broadcasting) : (10, 5)\n",
      "naive_add_matrix_and_vector(a, b) : (10, 5)\n",
      "\n",
      "c : (64, 3, 32, 10)\n",
      "d : (32, 10)\n",
      "c+d (broadcasting) : (64, 3, 32, 10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 직접 구현한 broadcasting\n",
    "# 실제로는 알고리즘 수준에서 더 빠르게 구현되어 있을 것임, 간단히 구현만 해본 것\n",
    "import numpy as np\n",
    "\n",
    "def naive_add_matrix_and_vector(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[j]\n",
    "            \n",
    "    return x\n",
    "\n",
    "\n",
    "# 마지막 차원 맞춰주기 (5를 같게함)\n",
    "a = np.random.randint(-10, 10, [10, 5])\n",
    "b = np.random.randint(-10, 10, [5])\n",
    "c = np.random.randint(10, size=[64, 3, 32, 10])\n",
    "d = np.random.randint(10, size=[32, 10])\n",
    "\n",
    "print(\"a :\", a.shape)\n",
    "print(\"b :\", b.shape)\n",
    "print(\"\")\n",
    "\n",
    "print(\"a+b (broadcasting) :\", (a+b).shape)\n",
    "print(\"naive_add_matrix_and_vector(a, b) :\", naive_add_matrix_and_vector(a, b).shape)\n",
    "print(\"\")\n",
    "\n",
    "# 이렇게도 broadcasting이 일어남\n",
    "print(\"c :\", c.shape)\n",
    "print(\"d :\", d.shape)\n",
    "print(\"c+d (broadcasting) :\", (c+d).shape)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.3 텐서 점곱 (dot product)\n",
    "- 원소별 연산과 달리 입력 텐서의 원소들을 결합시킴\n",
    "- 넘파이, 케라스는 dot연산자를 사용(텐서플로에서는 다름)\n",
    "- 일반적인 행렬 곱셈 연산이랑 같음\n",
    "- A dot product B에서, (A의 행길이 == B의 열길이)를 만족해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13. 16.]\n",
      " [29. 36.]]\n",
      "[[13 16]\n",
      " [29 36]]\n"
     ]
    }
   ],
   "source": [
    "# 파이썬으로 점곱 구현\n",
    "import numpy as np\n",
    "\n",
    "# dot(vector, vector)\n",
    "def naive_vv_dot(x, y):\n",
    "    assert len(x.shape) == 1\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    \n",
    "    z = 0.\n",
    "    for i in range(x.shape[0]):\n",
    "        z += x[i]*y[i]\n",
    "        \n",
    "    return z\n",
    "\n",
    "\n",
    "# dot(matrix, vector)\n",
    "def naive_mv_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(y.shape[0]):\n",
    "            z[j] += x[i][j]*y[j]\n",
    "            \n",
    "    return z\n",
    "\n",
    "\n",
    "# dot(matrix, vector)\n",
    "def naive_mv_dot2(x, y):\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        z[i] = naive_vector_dot(x[i][:], y)\n",
    "        \n",
    "    return z\n",
    "\n",
    "\n",
    "# dot(matrix, matrix)\n",
    "def naive_mm_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 2\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    z = np.zeros((x.shape[0], y.shape[1]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x_row = x[i,:]\n",
    "            y_col = y[:,j]\n",
    "            z[i, j] = naive_vv_dot(x_row, y_col)\n",
    "            \n",
    "    return z\n",
    "    \n",
    "\n",
    "a = np.array([[1, 2],[3, 4]])\n",
    "b = np.array([[3, 4],[5, 6]])\n",
    "\n",
    "print(naive_mm_dot(a, b))\n",
    "print(np.dot(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.4 텐서 크기 변환 (tensor reshaping)\n",
    "- 특정 크기에 맞게 열과 행을 재배열 하는 것\n",
    "- 주로 전치(transposition)를 사용함\n",
    "- reshape()를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [2. 3.]\n",
      " [4. 5.]]\n",
      "\n",
      "[[0. 1. 2.]\n",
      " [3. 4. 5.]]\n",
      "\n",
      "[[0. 2. 4.]\n",
      " [1. 3. 5.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[0., 1.], [2., 3.], [4., 5.]])\n",
    "print(x)\n",
    "print(\"\")\n",
    "\n",
    "# reshape를 통해 형태 transposition이랑 같게 해도 원소 배치는 다름\n",
    "# reshape는 형태만 재배열하는것\n",
    "y = x.reshape((x.shape[1], x.shape[0]))\n",
    "# transposition은 x[:, i] => x[i, :]\n",
    "z = np.transpose(x)\n",
    "\n",
    "print(y)\n",
    "print(\"\")\n",
    "\n",
    "print(z)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.5 텐서 연산의 기하학적 해석\n",
    "- 텐서 연산이 조작하는 텐서의 내용은 어떤 기하학적 공간에 있는 좌표 포인트로 해석할 수 있음\n",
    "- 따라서 모든 텐서 연산은 기하학적 해석이 가능함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5  1.25]\n",
      "\n",
      "[1. 1.]\n",
      "[-1.  1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ex1\n",
    "# 벡터의 덧셈은 기히학적으로 화살표 벡터의 연결이라 할 수 있음\n",
    "A = np.array([0.5, 1])\n",
    "B = np.array([1, 0.25])\n",
    "print(A+B)\n",
    "print(\"\")\n",
    "\n",
    "# Ex2\n",
    "# 행렬 [[cos(theta), sin(theta)],[-sin(theta), cos(theta)]]의 dot product는 \n",
    "# theta 각도로 2D 벡터를 회전하는 것이라 할 수 있음\n",
    "\n",
    "v = np.array([1., 1.])\n",
    "theta = np.pi / 2.\n",
    "R = np.array([[np.cos(theta), np.sin(theta)],[-np.sin(theta), np.cos(theta)]])\n",
    "\n",
    "print(v)\n",
    "print(np.dot(v, R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.6 딥러닝의 기하학적 해석\n",
    "신경망은 전체적으로 텐서 연산의 연결들로 구성되고 모든 텐서 연산은 입력 데이터의 기하학적 변환이다. 따라서 단순한 단계들이 길게 이어저 구현된 신경망을 고차원 공간에서 매우 복잡한 기하학적 변환을 하는 것으로 해석할 수 있다.\n",
    "\n",
    "3D로 비유하자면, 하나는 빨강, 하나는 파랑인 2개의 색종이가 있다고 가정하고 두 장을 겹친 후 뭉쳐서 작은 공으로 만든다면, 이 종이 공이 입력 데이터, 색종이는 분류 문제의 데이터 클래스라고 할 수 있다.\n",
    "여기서 신경망이 해야 할 일은 종이 공을 펼쳐서 두 클래스가 다시 깔끔하게 분리되는 변환을 찾는 것이다.\n",
    "\n",
    "**종이 공을 하나하나 펼치며 복잡한 분해 과정을 처리하는 것 = 심층 신경망의 연결된 각 층에서의 간단한 변환으로 복잡한 데이터를 풀어주는 것**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
