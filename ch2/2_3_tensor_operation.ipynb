{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 텐서 연산\n",
    "\n",
    "- 첫 예제에서의 Dense layer를 간단히 분석해보면 다음과 같다.\n",
    "\n",
    "\n",
    "- keras.layers.Dense(512, activation='relu')\n",
    "    - 입력으로 2D 텐서를 받아서 출력으로 2D 텐서를 반환하는 함수\n",
    "    - 더 자세히 풀어서 보면 다음과 같다.\n",
    "    \n",
    "    \n",
    "- output = relu(dot(W, input) + b)\n",
    "    - input(입력 2D 텐서), W(가중치 2D 텐서), bias(벡터)\n",
    "    - 총 3가지 연산을 함\n",
    "        1. input과 W의 dot product\n",
    "        2. 1의 결과(2D 텐서)와 b의 덧셈\n",
    "        3. ReLU 연산\n",
    "    \n",
    "    \n",
    "- ReLU 연산\n",
    "    - max(x, 0)\n",
    "    - 입력이 0보다 크면 그대로 반환\n",
    "    - 0보다 작으면 0을 반환\n",
    "    \n",
    "    \n",
    "***Sequential 클래스의 add() method에 Dense 클래스가 추가될 때, Dense 객체의 build() 메서드가 호출되며 가중치(kernel) W와 편향(bias) b가 생성되고, Dense 객체의 kernel과 bias 인스턴스 변수에 저장됨***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 원소별 연산 (element-wise operation)\n",
    "\n",
    "- relu()와 덧셈은 원소별 연산\n",
    "- 원소별 연산이란 텐서내의 원소끼리 이루어지는 연산을 뜻함\n",
    "- 넘파이에 원소별 연산이 이미 구현되어 있고 훨씬 빠름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "[[-3  9 -9]\n",
      " [-7 -9  6]\n",
      " [-2  4  2]]\n",
      "b\n",
      "[[ -3  -7 -10]\n",
      " [ -5   8   3]\n",
      " [  9   3  -8]]\n",
      "\n",
      "relu(a)\n",
      "[[0 9 0]\n",
      " [0 0 6]\n",
      " [0 4 2]]\n",
      "\n",
      "add(a, b)\n",
      "[[ -6   2 -19]\n",
      " [-12  -1   9]\n",
      " [  7   7  -6]]\n",
      "\n",
      "numpy의 relu\n",
      "[[0. 9. 0.]\n",
      " [0. 0. 6.]\n",
      " [0. 4. 2.]]\n",
      "\n",
      "numpy의 add\n",
      "[[ -6   2 -19]\n",
      " [-12  -1   9]\n",
      " [  7   7  -6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 직접 구현한 relu()\n",
    "def naive_relu(x):\n",
    "    assert len(x.shape) == 2\n",
    "    \n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = max(x[i, j], 0)\n",
    "            \n",
    "    return x\n",
    "\n",
    "\n",
    "# 직접 구현한 add()\n",
    "def naive_add(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert x.shape == y.shape\n",
    "    \n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[i, j]\n",
    "            \n",
    "    return x\n",
    "\n",
    "\n",
    "a = np.random.randint(-10, 10, (3, 3))\n",
    "b = np.random.randint(-10, 10, (3, 3))\n",
    "\n",
    "print(\"a\")\n",
    "print(a)\n",
    "print(\"b\")\n",
    "print(b)\n",
    "print(\"\")\n",
    "\n",
    "print(\"relu(a)\")\n",
    "print(naive_relu(a))\n",
    "print(\"\")\n",
    "\n",
    "print(\"add(a, b)\")\n",
    "print(naive_add(a, b))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# numpy의 구현된 maximum(relu()와 동일한 기능)\n",
    "print(\"numpy의 relu\")\n",
    "print(np.maximum(a, 0.))\n",
    "print(\"\")\n",
    "\n",
    "# numpy의 구현된 add()\n",
    "print(\"numpy의 add\")\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2 브로드캐스팅\n",
    "- 작은 텐서가 큰 텐서의 크기에 맞춰지는 것을 브로드캐스팅이라 함\n",
    "- 브로드캐스팅의 2단계\n",
    "    1. 큰 텐서의 ndim에 맞도록 작은 텐서에 축(브로드캐스팅 축이라 함)이 추가됨\n",
    "    2. 작은 텐서가 새 축을 따라서 큰 텐서의 크기에 맞도록 반복됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a : (10, 5)\n",
      "b : (5,)\n",
      "\n",
      "a+b (broadcasting) : (10, 5)\n",
      "naive_add_matrix_and_vector(a, b) : (10, 5)\n",
      "\n",
      "c : (64, 3, 32, 10)\n",
      "d : (32, 10)\n",
      "c+d (broadcasting) : (64, 3, 32, 10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 직접 구현한 broadcasting\n",
    "# 실제로는 알고리즘 수준에서 더 빠르게 구현되어 있을 것임, 간단히 구현만 해본 것\n",
    "def naive_add_matrix_and_vector(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[j]\n",
    "            \n",
    "    return x\n",
    "\n",
    "\n",
    "# 마지막 차원 맞춰주기 (5를 같게함)\n",
    "a = np.random.randint(-10, 10, [10, 5])\n",
    "b = np.random.randint(-10, 10, [5])\n",
    "c = np.random.randint(10, size=[64, 3, 32, 10])\n",
    "d = np.random.randint(10, size=[32, 10])\n",
    "\n",
    "print(\"a :\", a.shape)\n",
    "print(\"b :\", b.shape)\n",
    "print(\"\")\n",
    "\n",
    "print(\"a+b (broadcasting) :\", (a+b).shape)\n",
    "print(\"naive_add_matrix_and_vector(a, b) :\", naive_add_matrix_and_vector(a, b).shape)\n",
    "print(\"\")\n",
    "\n",
    "# 이렇게도 broadcasting이 일어남\n",
    "print(\"c :\", c.shape)\n",
    "print(\"d :\", d.shape)\n",
    "print(\"c+d (broadcasting) :\", (c+d).shape)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
